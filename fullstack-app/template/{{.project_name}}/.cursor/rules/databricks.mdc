---
description: Databricks SDK and Lakebase integration patterns
globs:
  - "src/api/**/*.py"
  - "**/databricks*.py"
  - "**/lakebase*.py"
alwaysApply: false
---

# Databricks Integration Guidelines

## Configuration

- Use `settings.databricks_config` for WorkspaceClient config
- Use `settings.instance_name` for Lakebase instance name
- Check `settings.is_deployed` for environment detection

```python
from core import settings

# Get configured WorkspaceClient
ws = WorkspaceClient(config=settings.databricks_config)

# Lakebase config is resolved via SDK in DatabricksService
# using settings.instance_name
```

## SQL Safety

- Use parameterized queries, never string interpolation for values
- Use escape functions for dynamic table/column names

```python
# Good - parameterized query
backend.fetch("SELECT * FROM users WHERE id = %s", (user_id,))

# Good - escaped identifier
from clients import escape_full_name
table = escape_full_name("catalog.schema.table")
backend.execute(f"SELECT * FROM {table}")

# Bad - SQL injection risk
backend.fetch(f"SELECT * FROM users WHERE id = {user_id}")
```

## Authentication

- In deployed mode: use per-user tokens from X-Forwarded-Access-Token
- In local mode: use .env file with DATABRICKS_HOST/TOKEN or DATABRICKS_PROFILE
- OAuth tokens refresh automatically via OAuthTokenManager

## Service Pattern

```python
from services import DatabricksService

service = DatabricksService()

# SQL Warehouse queries
rows = service.sql_backend.fetch("SELECT * FROM table")

# Lakebase (PostgreSQL) queries
rows = service.lakebase_backend.fetch("SELECT * FROM table")

# Direct SDK access
user = service.workspace_client.current_user.me()
```